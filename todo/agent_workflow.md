# Agent Workflow & Orchestration

**Overview:** This document outlines the end-to-end reasoning process of the AI agent. The agent orchestrates the use of the criteria parser and FHIR search tools to evaluate trial eligibility for a given patient. We leverage Bedrock’s capabilities (or a similar framework) to enable the agent to plan actions (tool calls) and carry state between steps, ensuring a coherent multi-step workflow.

## Tasks

* \[ \] **Define input/output API** – Determine how the agent is invoked and what it returns. For example, the agent could be triggered with a specific patient ID and trial ID (or the text of criteria) as input. The expected output could be a structured response indicating eligibility (true/false or percentage match) and a justification (which criteria were or were not met). Document this interface in the README and ensure the demo will follow this format.

* \[ \] **Select agent approach** – Decide between using **Amazon Bedrock AgentCore** or a custom-built agent loop:

* If using *Bedrock AgentCore*, set up an Agent (using the Agent Toolkit or JSON definitions) that includes our two tools (parser and search) and a predefined prompt. AgentCore provides primitives like secure runtime isolation and can handle the decision-making loop for calling tools.

* If custom (e.g., using LangChain-style ReAct logic), implement a loop where the LLM is called to decide an action (parse criteria or check a criterion) and your code executes the tool, then feeds the result back into the LLM for the next step. Document the reasoning for the choice. (AgentCore is preferred for managed orchestration and to meet hackathon recommendations[\[12\]](https://aws-agent-hackathon.devpost.com/rules#:~:text=2,DIY%20agents%20using%20AWS%20infrastructure).)

* \[ \] **Tool integration via AgentCore** – Register the **Criteria Parser** and **FHIR Search** functionalities as tools in the agent’s environment. Using AgentCore’s Gateway, wrap the parser Lambda and FHIR query Lambda as API endpoints that the agent can call securely[\[13\]](https://aws.amazon.com/bedrock/agentcore/#:~:text=Enhance%20agents%20with%20tools%20and,memory)[\[3\]](https://blog.radixia.ai/serverless-ai-agents-with-amazon-bedrock-agentcore/#:~:text=Amazon%20Bedrock%20AgentCore%20is%20structured,The%20microVMs%20are). Verify the tool schemas (input/output) are correctly defined so the agent knows how to invoke them. For example, in AgentCore you might define a tool with name “ParseCriteria” that calls a specific URL/ARN and expects a JSON response.

* \[ \] **Design agent prompt** – Write the system prompt and few-shot examples for the agent’s reasoning. The prompt should detail the task: e.g., *“You are an AI agent that helps determine if a patient is eligible for a clinical trial. You have access to the following tools: ParseCriteria (to convert trial criteria text into structured form) and CheckCriterion (to verify if the patient meets a specific criterion using the patient’s data). Your job is to use these tools step-by-step and then provide a final answer.”* Include an example workflow in the prompt if possible: a simple scenario of using the tools and arriving at an answer, to illustrate the desired format and behavior.

* \[ \] **Implement control flow** – Write the code or configuration that runs the agent through the steps:

* **Parse criteria** – The agent (or orchestrating code) first calls the ParseCriteria tool on the trial’s criteria text (if criteria text is the input, or retrieved from a database).

* **Iterate criteria checks** – For each structured criterion returned, the agent calls the FHIR search tool to check the patient’s data. The agent should collect the results (met or not met for each).

* **Decision and output** – The agent evaluates the overall eligibility. If any **inclusion** criterion is not met or any **exclusion** criterion *is* met, the patient is ineligible. Otherwise, eligible. The agent then formulates the output, e.g., *“Patient* *is not eligible* *because they do not meet the following criteria: ...”* or *“Patient* *is eligible* *for the trial. All inclusion criteria satisfied and no exclusion criteria present.”* This reasoning can be directly generated by the LLM based on tool results, guided by the prompt to cite which criteria failed or passed. Implement this logic either in the final LLM prompt (having the LLM list out the criteria status) or in code after tool calls. Ensure the agent’s final answer is easy to understand.

* \[ \] **Memory and state** – Determine if the agent needs to maintain state. For a single turn (one patient-trial evaluation), short-term memory (the context of the LLM) should suffice to carry the list of criteria and their status. If we wanted the agent to handle multi-turn conversations (like a user asking follow-up questions), we could use AgentCore’s memory primitive to store conversation state[\[13\]](https://aws.amazon.com/bedrock/agentcore/#:~:text=Enhance%20agents%20with%20tools%20and,memory). For this hackathon scope, a single-turn decision is okay, but note in documentation that the architecture could be extended to interactive use with memory.

* \[ \] **Testing the agent** – Conduct a full test with an example trial and patient:

* Provide the trial criteria text to the agent (or have it fetched if we stored it somewhere).

* Provide the patient ID.

* Run the agent and observe the sequence of actions. Ensure: the parser is called once and returns a sensible structured output; then the search tool is called for each criterion and returns correct booleans; then the agent’s final message correctly reflects those results.

* Check for any logical errors (e.g., agent misinterpreting a tool result). If the agent is LLM-driven, sometimes it might misunderstand the tool output schema – adjust the prompt or tool interface to make it foolproof (for example, keep tool outputs simple, like "result": true/false).

* \[ \] **Iterate and refine** – Based on tests, refine the workflow. Maybe the agent needs more explicit instructions or the tools need tweaks in how they return data. Ensure the agent doesn’t make unnecessary tool calls (like parsing criteria every time it checks one criterion – it should parse once, then loop through results; the prompt or agent design should enforce this).

* \[ \] **Implement guardrails in workflow** – Besides the general guardrails (like content filtering) enabled in Bedrock, consider adding any domain-specific guardrails. For example, if the agent somehow tries to fetch data it shouldn’t (like a different patient’s data), ensure our tool only allows the intended patient ID. If using a custom loop, put checks so the agent doesn’t call tools in a harmful order (though in this scope, the tools are fairly safe). Essentially, **validate the agent’s action outputs**: since we control the code that executes tool calls, we can ignore or sandbox any unexpected requests.

* \[ \] **Parallel or batch processing (Stretch)** – For scalability, consider that in real scenarios one might want to check one patient against many trials or vice versa. Outline how the agent could be extended: e.g., loop over multiple trials by repeatedly using the parser on each trial and checking the patient. This might be beyond a single agent session’s scope (could use external orchestration like a Step Function). While not implementing fully, we can mention this in documentation as a next step.

* \[ \] **Frontend or API integration (Stretch)** – If time permits, implement a simple front-end to invoke the agent. This could be a Streamlit app or a static webpage with JavaScript calling our API. Alternatively, create a CLI script. The idea is to make it easy for a user (or judge) to run the agent with minimal fuss. For example, a small web form where you select a patient and a trial from dropdowns and then see the result. This is a nice-to-have to elevate the presentation quality, but not required if time is short (the focus remains the backend agent functionality).

---

